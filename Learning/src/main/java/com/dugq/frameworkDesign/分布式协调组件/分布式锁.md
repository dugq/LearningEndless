# 分布式锁
#### 分布式锁的特性
* 原子性（加锁/解锁）
* 唯一性
* 安全性
  * 在完成加锁动作后：
    * 服务可用时间范围内锁持续有效
    * 服务异常宕机锁自动释放
    * 解锁动作必须只能被加锁线程操作
* 可重入性
* 可重试

## redis
###### 加锁
~~~lua
  -- 不存在，可获取锁，存在同一线程的，可获取重入
  if ((redis.call('exists', KEYS[1]) == 0)  
   or (redis.call('hexists', KEYS[1], ARGV[2]) == 1)) then
    -- hincrby 实现可重入
    redis.call('hincrby', KEYS[1], ARGV[2], 1);
    redis.call('pexpire', KEYS[1], ARGV[1]);
    return nil;
  end;
    return redis.call('pttl', KEYS[1]);
   KEYS[1]： key
   ARGV[1]：过期时间
   ARGV[2]: servierId + 线程ID
   
~~~
* 利用lua脚本配合redis的原子性实现加锁的原子性
* 使用exists + redis命令的有序性实现唯一性判断
* key设置有效期，避免异常宕机导致的死锁
* 利用hash结构
  * key为 serviceId(uuid)+线程ID 实现不同节点，不同线程的唯一标识
  * value 为 数字类型，利用hincrby 实现重入计数
~~~lua watcherDog 定时任务
if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then 
  redis.call('pexpire', KEYS[1], ARGV[1]); 
  return 1; 
end; 
return 0;
  KEYS[1]： key
   ARGV[1]：过期时间
   ARGV[2]: servierId + 线程ID
~~~
* 定期刷新key，保证key的有效性
* 如果自行设置了key的有效期，那么redisson将不提供刷新key的策略，即使未解锁

###### 解锁
~~~lua
if (redis.call('hexists', KEYS[1], ARGV[3]) == 0) then 
  return nil;
end; 
local counter = redis.call('hincrby', KEYS[1], ARGV[3], -1); 
if (counter > 0) then 
  redis.call('pexpire', KEYS[1], ARGV[2]); 
  return 0; 
else 
  redis.call('del', KEYS[1]); 
  redis.call(ARGV[4], KEYS[2], ARGV[1]); 
  return 1; 
end; 
return nil;

key[1] :  key
key[2] :  redisson_lock_channel + key
ARGV[1] : publish_msg
ARGV[2] : 有效期
ARGV[3] : serverId+threadId
~~~
* 利用lua脚本保证整个过程的原子性
* 利用exist key hkey 判断保证 服务 + 线程 的唯一性
* 通过hincrby key hkey -1 保证重入锁的依次解锁
## zk
###### 加锁
~~~java
private static class LockData {
    final Thread owningThread;
    final String lockPath;
    final AtomicInteger lockCount = new AtomicInteger(1);
}
public class InterProcessMutex{
  private final ConcurrentMap<Thread, LockData> threadData = Maps.newConcurrentMap();
  
  public void acquire(){
    Thread currentThread = Thread.currentThread();

    LockData lockData = threadData.get(currentThread);
    if ( lockData != null )
    {
      // re-entering
      lockData.lockCount.incrementAndGet();
      return true;
    }
    lockPath = client.create().creatingParentContainersIfNeeded().withProtection().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(path, lockNodeBytes);
    // 利用watch 循环尝试获取锁
    internalLockLoop(startMillis, millisToWait, ourPath);
    if ( lockPath != null )
    {
      LockData newLockData = new LockData(currentThread, lockPath);
      threadData.put(currentThread, newLockData);
      return true;
    }
  }
}
~~~
* 利用zk的命名服务实现唯一性
* 利用zk的临时文件的权限机制实现不同机器的安全性
* 利用zk的临时文件实现宕机自动释放锁
* 锁的信息本地也有保存。可重入便是在本地实现
* 利用watcher机制实现重试
###### 解锁
~~~java
public class InterProcessMutex{
    public void release() throws Exception{
        /*
            Note on concurrency: a given lockData instance
            can be only acted on by a single thread so locking isn't necessary
         */

        Thread currentThread = Thread.currentThread();
        LockData lockData = threadData.get(currentThread);
        if ( lockData == null ) {
            throw new IllegalMonitorStateException("You do not own the lock: " + basePath);
        }
        int newLockCount = lockData.lockCount.decrementAndGet();
        if ( newLockCount > 0 ) {
            return;
        }
        if ( newLockCount < 0 ) {
            throw new IllegalMonitorStateException("Lock count has gone negative for lock: " + basePath);
        }
        try {
          client.delete().guaranteed().forPath(ourPath);
        }
        finally {
            threadData.remove(currentThread);
        }
    }
}
~~~
* 解锁优先释放重入锁的内部锁
* 当最外层解锁时，再删除zk临时文件

|       | 原子性            | 唯一性     | 续期            | 宕机自动释放    | 线程安全              | 可重入             | 可重试          |   
|:------|----------------|---------|---------------|-----------|-------------------|-----------------|--------------|
| redis | 单个命令或者lua脚本    | key的唯一性 | 异步定时任务死循环刷新   | 为key设置有效期 | 将线程ID设置为hashkey   | hashvalue记录重入数量 | 利用发布订阅模式不断重试 |
| zk    | 利用临时文件的权限跳过原子性 | 文件名服务   | 临时文件的断开重联跳过续期 | 临时文件的特性   | 本地锁空间存储线程ID实现线程隔离 | 本地实现            | watcher机制    |

# redis vs zk 对比

|          | redis                                    | zk                  |
|----------|------------------------------------------|---------------------|
| 集群模式的一致性 | AP模型，有问题，但加锁利用的写命令，解锁用的lua脚本，所以理论都是主节点执行 | CP模型，理论情况下，一致性问题出现概率较低 |
| 连接不稳定问题  |                       |            |
|          |                                          |                     |

# 连接不稳定问题

|                  | redis                     | zk                                   |
|------------------|---------------------------|--------------------------------------|
| 持有锁的客户端异常宕机      | 其他客户端需要等到key失效后才能获取到锁     | 在session过期后（10s左右）才能获取到锁             |
| 持有锁的客户端连接异常断开后重联 | key有效期内不影响，key有效期外，存在并发问题 | session有效期内不影响，session有效期外，存在并发问题    |
| 持有锁的客户端异常重启      | 基本无影响                     | 基本无影响                                |
| 排队的客户端连接异常断开     | 基本无影响                     | 重联机制触发，则排队继续。非重联机制等同于宕机              |
| 排队的客户端宕机         | 基本无影响                     | 排在该客户端后面的节点需要等到宕机节点的session过期后才能获取到锁 | 
